import asyncio
import logging
import aiodns
from datetime import datetime, UTC
from urllib.parse import urlparse

from ..db_manager import (
    get_all_indicators_iter,
    delete_whitelisted_indicators,
    db_transaction,
    get_sources_for_indicator,
    get_domains_for_resolution,
    update_dns_cache_batch,
    get_existing_ips,
    delete_indicators,
    get_dns_resolution_cache_iter
)
from ..database.connection import DB_WRITE_LOCK

logger = logging.getLogger(__name__)

async def resolve_domain(resolver, domain):
    try:
        # A record lookup
        result = await resolver.query(domain, 'A')
        return [r.host for r in result]
    except Exception:
        return []

def extract_domain(indicator, itype):
    if itype == 'url':
        try:
            parsed = urlparse(indicator)
            if parsed.hostname:
                return parsed.hostname
        except:
            pass
        return indicator.split('/')[0]
    return indicator

async def process_background_dns_batch(batch_size=50):
    """
    Background task: Only resolves domains and updates the cache.
    Does NOT delete anything. Deletion is handled by run_deduplication_sweep.
    """
    # 1. Get candidates (Domains not resolved recently)
    candidates = get_domains_for_resolution(limit=batch_size, retry_days=7)
    
    if not candidates:
        return 0
        
    # logger.info(f"DNS Batch: Resolving {len(candidates)} domains...")

    # 2. Resolve Async
    loop = asyncio.get_running_loop()
    resolver = aiodns.DNSResolver(loop=loop)
    
    tasks = []
    items_to_process = []
    
    for item in candidates:
        original = item['indicator']
        itype = item['type']
        domain = extract_domain(original, itype)
        
        items_to_process.append({
            'original': original,
            'domain': domain
        })
        tasks.append(resolve_domain(resolver, domain))
    
    results = await asyncio.gather(*tasks)
    
    # 3. Prepare Data for Cache
    cache_updates = []
    now_iso = datetime.now(UTC).isoformat()
    
    for idx, ips in enumerate(results):
        item = items_to_process[idx]
        
        # Store IPs as comma-separated string
        ip_str = ",".join(ips)
        cache_updates.append({
            'domain': item['original'], 
            'resolved_ips': ip_str,
            'last_resolved': now_iso
        })
        
    # 4. Update Cache Only
    if cache_updates:
        update_dns_cache_batch(cache_updates)
        
    return len(candidates)

def run_deduplication_sweep():
    """
    Core Logic: 
    1. Loads ALL known malicious IPs from DB.
    2. Iterates through the DNS Resolution Cache.
    3. If a cached domain resolves to a malicious IP, delete the domain.
    """
    logger.info("Starting DNS Deduplication Sweep (Cache vs IP List)...")
    
    # 1. Load all Threat IPs into memory (efficient Set)
    threat_ips = set()
    for row in get_all_indicators_iter():
        if row['type'] == 'ip':
            threat_ips.add(row['indicator'])
            
    if not threat_ips:
        logger.info("Deduplication Sweep: No IP indicators found to check against.")
        return 0

    # 2. Iterate Cache and Check
    domains_to_delete = []
    scanned_count = 0
    
    # Batch delete to manage memory
    BATCH_DELETE_SIZE = 1000
    total_deleted = 0
    
    cache_iter = get_dns_resolution_cache_iter()
    
    for row in cache_iter:
        domain = row['domain']
        resolved_ips_str = row['resolved_ips']
        scanned_count += 1
        
        if not resolved_ips_str:
            continue
            
        # Parse IPs from CSV
        resolved_ips = resolved_ips_str.split(',')
        
        # Check intersection
        # If ANY resolved IP is in our threat_ips set
        if any(ip in threat_ips for ip in resolved_ips):
            domains_to_delete.append(domain)
            
        if len(domains_to_delete) >= BATCH_DELETE_SIZE:
            count = delete_indicators(domains_to_delete)
            total_deleted += count
            domains_to_delete = []
            
    # Final Flush
    if domains_to_delete:
        count = delete_indicators(domains_to_delete)
        total_deleted += count
        
    logger.info(f"Deduplication Sweep Complete. Scanned {scanned_count} cached domains. Removed {total_deleted} duplicates.")
    return total_deleted